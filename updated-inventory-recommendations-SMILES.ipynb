{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5ca57b1",
   "metadata": {},
   "source": [
    "ok so in this notebook, i make acute lil embedder for all the big molecules. \n",
    "what you need:\n",
    "- tmc1 inventory in selfies, already embedded\n",
    "- datasets from pubchem in selfies\n",
    "\n",
    "this script will ideally:\n",
    "- embed full datasets with VICGAE\n",
    "- drop any duplicates\n",
    "- filter molecules for not hydrogen and contains anything we dont want from keep_elements\n",
    "\n",
    "your outcome is a .csv of your recommendations, minus all the duplicates/weirdos like Te or something, and a .smi that you'll put through psi4 later for calculations. \n",
    "Note that this code is for SMILES and not SELFIES. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "615d1a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#associated with VICGAE\n",
    "from typing import Tuple, Dict\n",
    "import warnings\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import numpy as np\n",
    "import joblib\n",
    "from joblib import load, dump\n",
    "\n",
    "import selfies as sf\n",
    "from astrochem_embedding import VICGAE, get_paths, get_pretrained_path, Translator\n",
    "import h5py\n",
    "import dask\n",
    "import pandas as pd\n",
    "from periodictable import elements\n",
    "from dask_ml.metrics import pairwise_distances\n",
    "#from sklearn.metrics.pairwise import pairwise_distances\n",
    "import dask.dataframe as dd\n",
    "from tqdm.auto import tqdm\n",
    "from common import encode_smiles, map_to_embeddings, bootstrap\n",
    "import dask.array as da\n",
    "import numpy.core.defchararray as np_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "578f7b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose what elements you want to keep...\n",
    "keep_elements = [\"H\", \"C\", \"O\", \"N\", \"P\" , \"S\", \"Si\"]\n",
    "element_filter = [el.symbol for el in elements if el.symbol not in keep_elements]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a6ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedder, VICGAE, is already trained on a global set of molecules\n",
    "#multipurpose\n",
    "embedding_model = VICGAE.from_pretrained()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2048b3b8",
   "metadata": {},
   "source": [
    "## load tmc1 inventory and matching regressor pickle\n",
    "My inventory is already embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f730c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is embedded\n",
    "#maybe... not needed\n",
    "#detected_list = pd.read_hdf(\"tmc1_embedded.h5\", \"data\")#tmc-1 detected inventory in embedded selfies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7fc6337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorized tmc-1\n",
    "\n",
    "dftmc1 = pd.read_csv(r'FI_updatedjan2025.csv')\n",
    "#dftmc1 = pd.read_csv(r'OI_revisedjan2025.csv')\n",
    "#dftmc1 = pd.read_csv(r'OD_updated_corrected.csv')\n",
    "#dftmc1 = pd.read_csv(r'fd_UPDATED_jan2025.csv')\n",
    "\n",
    "\n",
    "#dftmc1 = pd.read_csv(r'grouped-TMC1_full_Inventory_selfies_NOIsotopes_noCO.csv')\n",
    "#grouped-TMC1_full_Inventory_selfies_NOIsotopes_noCO\n",
    "#grouped-TMC1_ORIG_Inventory_selfies_NoIsotopes_NoCO.csv\n",
    "#grouped-TMC1_ORIG_Inventory_selfies_WITHIsotopes_noCO\n",
    "#grouped-TMC1_full_Inventory_selfies_WithIsotopes_noCO\n",
    "\n",
    "detected_smiles_array = dftmc1['SMILES']\n",
    "detected_selfies_array = dftmc1['SELFIES']\n",
    "tmc1_selfies = dftmc1['SELFIES'].astype('str').apply(map_to_embeddings).values #from dataframe df, in selfies column, apply functionm aptoembeddings\n",
    "vecs = np.vstack(tmc1_selfies) #this essentially makes the tensors into arrays, i think\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbaec4c",
   "metadata": {},
   "source": [
    "### get GP regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "36ab0fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hannahts/miniconda3/envs/daskRec/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator GaussianProcessRegressor from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gp_model = load(\"FI_updatedJan2025.pkl\")\n",
    "#gp_model = load(\"OD_updatedJan2025.pkl\")\n",
    "\n",
    "#gp_model = load(\"FD_updatedjan2025.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "#gp_model = load(\"oct2024test-gpr-full-INVENTORY-Without-isotopes_5.pkl\")\n",
    "#gp_model = load(\"final-gpr-originventory-withisotopes-groupkfold-noCO.pkl\")\n",
    "#final-gpr-ORIG-INVENTORY-Without-isotopes.pkl\n",
    "#final-gpr-full-INVENTORY-Without-isotopes.pkl\n",
    "#final-gpr-fullinventory-withisotopes-groupkfold-noCO.pkl\n",
    "#final-gpr-originventory-withisotopes-groupkfold-noCO.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8d97b5",
   "metadata": {},
   "source": [
    "## embed the full database set selfies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9fe89d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_database = dd.read_csv(r'./collected_selfies.csv/*.part')\n",
    "#full_database = dd.read_csv(r'./allSmilesIso_withSelfies.csv/*.part')\n",
    "full_database_smiles_array = np.array(full_database['SMILES'])\n",
    "full_database_selfies_array = np.array(full_database['SELFIES'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c48bc3",
   "metadata": {},
   "source": [
    "# now, load the embeddings\n",
    "i tested a few, so they're all here for history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dfc6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#above was run by A for me,\n",
    "#i'll load that here.\n",
    "\n",
    "#dataset = np.loadtxt(\"selfies.txt\") \n",
    "#dataset = np.load(\"/Users/hannahts/Dropbox (MIT)/embedded_selfies.npy\", allow_pickle=True)\n",
    "dataset = np.load(\"/Users/hannahts/Dropbox (MIT)/embedded_selfies_3Mdatasets.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "f07b2d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_database7m = dd.read_csv(r'./allSmilesIso_withSelfies.csv/*.part')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6ac0808",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dask=da.from_array(dataset, chunks=-1)\n",
    "#dataset is embedded selfies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7766d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell is really inefficnetly done but basically\n",
    "#i am removing all instance of None, tu rning it into an array, stacking the array, stacked again (dumb but too lazy to change)\n",
    "#then turning it into vectors\n",
    "\n",
    "dataset_none_gone =  [(i if i is not None else  np.zeros_like(dataset[0])) for i in dataset]\n",
    "new_dataset = np.array(dataset_none_gone) #np array\n",
    "data2set = np.vstack(new_dataset)\n",
    "data3set = np.vstack(data2set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeb7eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dask_noNone = da.from_array(data3set, chunks=-1) #vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dc5f7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.array.core.Array"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset_dask_noNone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe8369d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this cell for smiles, for direct use of astrochem_embeddings to embed from smiles to embeddings\n",
    "It SHOULD work for selfies as well, but this is not thoroughly tested...\n",
    "\"\"\"\n",
    "\n",
    "def is_not_pure_hydrogen(smi: str) -> bool:\n",
    "    # this function checks if the molecule only contains\n",
    "    # hydrogen, which stuffs up the obabel geometry generation\n",
    "    \n",
    "    try:\n",
    "        #print(\"working\", smi, type(smi))\n",
    "        if not smi:\n",
    "            return False\n",
    "        characters = list(set([c for c in smi if c.isalpha()]))\n",
    "#        characters = list(set([c for c in smi.tolist() if c.isalpha()]))\n",
    "        if (len(characters) == 1) and (characters[0] == \"H\"):\n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        print(\"Not working\", smi, f\"{smi.shape=}\", type(smi))\n",
    "\n",
    "def is_not_complex(smi: str) -> bool:\n",
    "    return \".\" not in smi\n",
    "#    return \".\" not in smi.tolist()\n",
    "\n",
    "\n",
    "# ##trying a new function\n",
    "# def is_over_25(smi: str) -> bool:\n",
    "#     if len(smi) > 25:\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a00d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09d7aed7",
   "metadata": {},
   "source": [
    "## finding indexes of molecules that are recommended by pairwise distance/distance in ml vector space to actual molecules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b09925",
   "metadata": {},
   "source": [
    "# getting recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "daf27cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f757830ebe4b4139afd82bf5b86aad8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = list()\n",
    "\n",
    "for tmc_index, vector in tqdm(enumerate(vecs), total=len(vecs)):\n",
    "    # we don't really care about the distance values, just the index\n",
    "    distances = pairwise_distances(dataset_dask_noNone, vector[None,:]).ravel().compute()\n",
    "    sorted_indices = np.argsort(distances) #ORIGINAL SCRIPT\n",
    "    #sorted_indices = np.argsort(np.absolute(distances)) #testing\n",
    "    \n",
    "    \n",
    "#     # get up to x number of recommendations, based on the :xxxx in sorted_indices\n",
    "\n",
    "    top_ten = sorted_indices[:100] #100 recs PER molecule in the TMC-1 inventory\n",
    "    #print(top_ten)\n",
    "    \n",
    "    for index in top_ten:\n",
    "\n",
    "\n",
    "\n",
    "        if detected_smiles_array[tmc_index] != full_database_smiles_array[index]:\n",
    "            results.append(\n",
    "                {\n",
    "                    \"entry\": index,\n",
    "                    \"SMILES_recommendation\": full_database_smiles_array[index],\n",
    "                    \"SELFIES_recommendation\": full_database_selfies_array[index],\n",
    "                    \"anchor\": detected_smiles_array[tmc_index],\n",
    "                    \"distance\": distances[index],\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a355829",
   "metadata": {},
   "source": [
    "## change results list into panda dataframe & filter as needed\n",
    "first we drop duplicates\n",
    "then we drop based on features in above cells--element filter, pure h, not complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9485b47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put results into pandas\n",
    "rec_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "79f1c52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "      <th>SMILES_recommendation</th>\n",
       "      <th>SELFIES_recommendation</th>\n",
       "      <th>anchor</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2251</td>\n",
       "      <td>C#CC#CCC#C</td>\n",
       "      <td>[C][#C][C][#C][C][C][#C]</td>\n",
       "      <td>CC#CC#CC#C</td>\n",
       "      <td>0.007621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2285</td>\n",
       "      <td>C#CC#CC#CC</td>\n",
       "      <td>[C][#C][C][#C][C][#C][C]</td>\n",
       "      <td>CC#CC#CC#C</td>\n",
       "      <td>0.022843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12462</td>\n",
       "      <td>CC#CC#CC#CC</td>\n",
       "      <td>[C][C][#C][C][#C][C][#C][C]</td>\n",
       "      <td>CC#CC#CC#C</td>\n",
       "      <td>0.035413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>461</td>\n",
       "      <td>C#CC#CCC</td>\n",
       "      <td>[C][#C][C][#C][C][C]</td>\n",
       "      <td>CC#CC#CC#C</td>\n",
       "      <td>0.037292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12466</td>\n",
       "      <td>C#CC#CCC#CC</td>\n",
       "      <td>[C][#C][C][#C][C][C][#C][C]</td>\n",
       "      <td>CC#CC#CC#C</td>\n",
       "      <td>0.037431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15637</th>\n",
       "      <td>3160023</td>\n",
       "      <td>[C-]#CC</td>\n",
       "      <td>[C-1][#C][C]</td>\n",
       "      <td>[2H]CC#CC#C</td>\n",
       "      <td>0.094283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15638</th>\n",
       "      <td>940678</td>\n",
       "      <td>N#CCF</td>\n",
       "      <td>[N][#C][C][F]</td>\n",
       "      <td>[2H]CC#CC#C</td>\n",
       "      <td>0.094669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15639</th>\n",
       "      <td>3160039</td>\n",
       "      <td>[C-]#CC#N</td>\n",
       "      <td>[C-1][#C][C][#N]</td>\n",
       "      <td>[2H]CC#CC#C</td>\n",
       "      <td>0.094767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15640</th>\n",
       "      <td>2898669</td>\n",
       "      <td>CC#CN</td>\n",
       "      <td>[C][C][#C][N]</td>\n",
       "      <td>[2H]CC#CC#C</td>\n",
       "      <td>0.094841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15641</th>\n",
       "      <td>2262486</td>\n",
       "      <td>C#CO</td>\n",
       "      <td>[C][#C][O]</td>\n",
       "      <td>[2H]CC#CC#C</td>\n",
       "      <td>0.095416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15642 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         entry SMILES_recommendation       SELFIES_recommendation  \\\n",
       "0         2251            C#CC#CCC#C     [C][#C][C][#C][C][C][#C]   \n",
       "1         2285            C#CC#CC#CC     [C][#C][C][#C][C][#C][C]   \n",
       "2        12462           CC#CC#CC#CC  [C][C][#C][C][#C][C][#C][C]   \n",
       "3          461              C#CC#CCC         [C][#C][C][#C][C][C]   \n",
       "4        12466           C#CC#CCC#CC  [C][#C][C][#C][C][C][#C][C]   \n",
       "...        ...                   ...                          ...   \n",
       "15637  3160023               [C-]#CC                 [C-1][#C][C]   \n",
       "15638   940678                 N#CCF                [N][#C][C][F]   \n",
       "15639  3160039             [C-]#CC#N             [C-1][#C][C][#N]   \n",
       "15640  2898669                 CC#CN                [C][C][#C][N]   \n",
       "15641  2262486                  C#CO                   [C][#C][O]   \n",
       "\n",
       "            anchor  distance  \n",
       "0       CC#CC#CC#C  0.007621  \n",
       "1       CC#CC#CC#C  0.022843  \n",
       "2       CC#CC#CC#C  0.035413  \n",
       "3       CC#CC#CC#C  0.037292  \n",
       "4       CC#CC#CC#C  0.037431  \n",
       "...            ...       ...  \n",
       "15637  [2H]CC#CC#C  0.094283  \n",
       "15638  [2H]CC#CC#C  0.094669  \n",
       "15639  [2H]CC#CC#C  0.094767  \n",
       "15640  [2H]CC#CC#C  0.094841  \n",
       "15641  [2H]CC#CC#C  0.095416  \n",
       "\n",
       "[15642 rows x 5 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fffda3c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rec_df['SMILES_recommendation'] = rec_df.apply(lambda r: r['SMILES_recommendation'] if type(rec_df['SMILES_recommendation'])!=float else np.nan, axis=1)\n",
    "rec_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b69e07b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_df['SMILES_recommendation'].isnull().values.any() #expect False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "465b94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# first filter out non-unique suggests by their database index\n",
    "#rec_df.drop_duplicates(\"entry\", inplace=True)\n",
    "\n",
    "# now filter out molecules that contain elements that are not in our keep list\n",
    "rec_df = rec_df.loc[\n",
    "    (~rec_df[\"SMILES_recommendation\"].apply(lambda x: any([el in x for el in element_filter]))) &\n",
    "    (rec_df[\"SMILES_recommendation\"].apply(is_not_complex)) & # removes complexes\n",
    "    (rec_df[\"SMILES_recommendation\"].apply(is_not_pure_hydrogen)) # removes pure hydrogen\n",
    "]\n",
    "rec_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "97f84aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df = rec_df.loc[(rec_df['SMILES_recommendation'].apply(lambda x: not \"[nH]\" in x))]\n",
    "#rec_df = rec_df.loc[(rec_df['recommendation'].apply(lambda x: not \"[nH]\" in x.tolist()))]\n",
    "#rec_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f61305a",
   "metadata": {},
   "source": [
    "## now, we should have a list of selfies strings.  let's predict their column densities!\n",
    "# now we run predictions for the column densities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d31e8b",
   "metadata": {},
   "source": [
    "since i just want embeddings, i think i can just go straight into embeddings...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4bc35989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMILES\n",
    "type(rec_df)\n",
    "recommendations_arr = [x for x in rec_df['SMILES_recommendation'].to_numpy()]\n",
    "\n",
    "ignored_list = []\n",
    "ignored_indices = []\n",
    "def safe_embeddings(smi):\n",
    "    try:\n",
    "        return embedding_model.embed_smiles(smi)\n",
    "    except:\n",
    "        ignored_list.append(smi)\n",
    "        \n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d8328a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfunctionally the same as\\n#rec_vecs = np.vstack([embedding_model.embed_smiles(smi) for smi in rec_df[\"recommendation\"]])\\n\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "functionally the same as\n",
    "#rec_vecs = np.vstack([embedding_model.embed_smiles(smi) for smi in rec_df[\"recommendation\"]])\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# rec_vecs = list()\n",
    "# for p in tqdm(range(len(rec_df))):\n",
    "#     veci = safe_embeddings(recommendations_arr[p])\n",
    "#     rec_vecs.append(veci)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8093d8",
   "metadata": {},
   "source": [
    "Ok, the next few lines should:\n",
    "1. remove duplicates (final_arr)\n",
    "2. vectorize/embed the array (final_vecs)\n",
    "3. Convert the SMILES from final_arr into SELFIES (final_arr_selfies)\n",
    "4. run embedded species through the model for cd\n",
    "5. also create lists for the anchor molecule and distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5c4a9384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes elements from ignored_list from recommendations_arr\n",
    "#also removes duplicates from inside recommendations_arr\n",
    "final_arr=list( set(recommendations_arr)^set(ignored_list) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f1b6c085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d537300bea454fb94a036bc614357a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1966 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_vecs = list()\n",
    "for p in tqdm(range(len(final_arr))):\n",
    "    veca = safe_embeddings(final_arr[p])\n",
    "    final_vecs.append(veca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cd6a40b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_arr_selfies = []\n",
    "for i in range(len(final_arr)):\n",
    "    \n",
    "    mol_ = sf.encoder(final_arr[i])\n",
    "    final_arr_selfies.append(mol_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2284151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making predictions\n",
    "columns, std = [], []\n",
    "for i in final_vecs:\n",
    "    _c, _s = gp_model.predict(i, return_std=True)\n",
    "    columns.append(_c)\n",
    "    std.append(_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "45bdfd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchorlist, distancelist = [], []\n",
    "for i in range(len(final_arr)):\n",
    "    _a = rec_df['anchor'][i]\n",
    "    _d = rec_df['distance'][i]\n",
    "    anchorlist.append(_a)\n",
    "    distancelist.append(_d)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7fa8bb",
   "metadata": {},
   "source": [
    "# now you can combine all the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7bf469de",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_full = np.column_stack([final_arr, final_arr_selfies, columns, std, anchorlist, distancelist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3f4d1059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn above numpy array into pandas for converting to csv below\n",
    "final_rec_df = pd.DataFrame(final_full, columns = ['SMILES','SELFIES','COLUMN DENSITY', 'STD', 'anchor', 'distance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e7ffa",
   "metadata": {},
   "source": [
    "# make sure to change name of csv!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6ed3b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns, std = gp_model.predict(rec_vecs, return_std=True)\n",
    "final_rec_df[\"gpr_column\"] = columns\n",
    "final_rec_df[\"uncertainty\"] = std\n",
    "final_rec_df.to_csv(\"summary_FI_JAN2025.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c78f12d",
   "metadata": {},
   "source": [
    "# make the .smi list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e512684",
   "metadata": {},
   "source": [
    "dont forget to change your file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7a6f67d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"targets-FI_jan2025.smi\", \"w+\") as write_file:\n",
    "    write_file.write(\"\\n\".join(final_rec_df[\"SMILES\"].tolist()))\n",
    "    \n",
    "    #targets.smi is full-inventory-no-isotopes\n",
    "    #targets-orig.smi original no isotope\n",
    "    #targets-full.smi full no isotope"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
